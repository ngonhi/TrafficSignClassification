{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_SIFT_ori","provenance":[],"mount_file_id":"1k_PttW0dYLs2b9tcCksbiKcysZLGU-Cy","authorship_tag":"ABX9TyNhAc2/H99LoykSnO1ABEHT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"HipHAzGXGxke","executionInfo":{"status":"ok","timestamp":1608970451695,"user_tz":-420,"elapsed":2194,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}}},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import os, glob\r\n","import pickle\r\n","import pandas as pd\r\n","import cv2 as cv\r\n","%matplotlib inline\r\n","\r\n","# classification required packages\r\n","\r\n","from sklearn.neural_network import MLPClassifier\r\n","from sklearn.model_selection import cross_val_score, train_test_split\r\n","from sklearn.metrics import classification_report\r\n","import joblib\r\n","\r\n","import warnings\r\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYthvlEvHNz1","executionInfo":{"status":"ok","timestamp":1608970475227,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}}},"source":["def load_data(filedir, filepath, csvfile):\r\n","    data = np.load(os.path.join(filedir, filepath), allow_pickle=True)\r\n","    train_info = pd.read_csv(os.path.join(filedir, csvfile))\r\n","    labels = np.array(train_info['ClassId'])\r\n","    \r\n","    return data, labels\r\n","\r\n","def randomize_data(data, labels):\r\n","    randomize = np.arange(len(labels))\r\n","    np.random.shuffle(randomize)\r\n","    X = data[randomize]\r\n","    y = labels[randomize]\r\n","    \r\n","    return X,y"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhxmiK1rHRKL","executionInfo":{"status":"ok","timestamp":1608970659261,"user_tz":-420,"elapsed":1173,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}}},"source":["data_dir = '/content/drive/MyDrive/GTSRB'\r\n","processed_train_path = '/content/drive/MyDrive/Kaggle/Sift&BowTest/Sift&BowPreprocess_train.npy'\r\n","processed_test_path = '/content/drive/MyDrive/Kaggle/Sift&BowTest/Sift&BowPreprocess_test.npy'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFca9bldH1T8","executionInfo":{"status":"ok","timestamp":1608970668337,"user_tz":-420,"elapsed":2687,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}},"outputId":"55d65515-4e6b-43d8-b542-a38a06e7ee23"},"source":["trainProcessedData, trainLabels = load_data(data_dir, processed_train_path, 'Train.csv')\r\n","trainProcessedData, trainLabels = randomize_data(trainProcessedData, trainLabels)\r\n","trainProcessedData.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39209, 150)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlsyJDKpH4Gz","executionInfo":{"status":"ok","timestamp":1608970686917,"user_tz":-420,"elapsed":3678,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}},"outputId":"a7154c44-ac76-4c10-d51a-b0192e23963a"},"source":["testProcessedData, testLabels = load_data(data_dir, processed_test_path, 'Test.csv')\r\n","testProcessedData.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(12630, 150)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"UVbkzmubIEMI","executionInfo":{"status":"ok","timestamp":1608970902498,"user_tz":-420,"elapsed":1022,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}}},"source":["estimators = [100,200,300,500]\r\n","def train_model_ori(estimators, trainData, trainLabels, testData, testLabels):\r\n","    for n in estimators:\r\n","        print(f'MLP WITH {n} HIDDEN LAYERS')\r\n","        if os.path.isfile(f\"/content/drive/MyDrive/Kaggle/clf/mlp_{str(n)}_sift_ori.pkl\"):\r\n","            print(\"[INFO] loading classifier: MLP trained on ori images...\")\r\n","            mlp = pickle.load(open(f\"/content/drive/MyDrive/Kaggle/clf/mlp_{str(n)}_sift_ori.pkl\", 'rb'))\r\n","            print(\"[INFO] Classifer is loaded as instance ::rf::\")\r\n","        else:\r\n","            print(\"[INFO] pre-trained classifier not found. \\n Training Classifier MLP\")\r\n","            mlp = MLPClassifier(n)\r\n","            print(mlp)\r\n","            mlp.fit(trainData,trainLabels)\r\n","            print(\"[INFO] Succefully trained the classsifier. \\n Saving the classifier for further use\")\r\n","            pickle.dump(mlp, open(f\"/content/drive/MyDrive/Kaggle/clf/mlp_{str(n)}_sift_ori.pkl\", 'wb')) \r\n","            print(\"[INFO] Classifier Saved\")\r\n","            \r\n","        predictions = mlp.predict(testData)\r\n"," \r\n","        # show a final classification report demonstrating the accuracy of the classifier\r\n","        print(\"EVALUATION ON TESTING DATA FOR\" + str(n) + 'MLP')\r\n","        print(classification_report(testLabels, predictions))\r\n","        \r\n","        print('ACCURACY of TRAINING DATA')\r\n","        print(mlp.score(trainData, trainLabels))\r\n","        print('-------------------------------------------------------')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EdfalPT8I5eJ","executionInfo":{"status":"ok","timestamp":1608971807954,"user_tz":-420,"elapsed":864345,"user":{"displayName":"Nhi Ngo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3hv8kT-LPPVVfov1HHd0nw5NDCvBdYck4b7v-Q=s64","userId":"16422571029059227305"}},"outputId":"590304f6-1ebf-4cac-b65c-bddaddb765ec"},"source":["train_model_ori(estimators, trainProcessedData, trainLabels, testProcessedData, testLabels)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["MLP WITH 100 HIDDEN LAYERS\n","[INFO] pre-trained classifier not found. \n"," Training Classifier MLP\n","MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=100, learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[INFO] Succefully trained the classsifier. \n"," Saving the classifier for further use\n","[INFO] Classifier Saved\n","EVALUATION ON TESTING DATA FOR100MLP\n","              precision    recall  f1-score   support\n","\n","           0       0.24      0.22      0.23        60\n","           1       0.65      0.60      0.62       720\n","           2       0.62      0.63      0.63       750\n","           3       0.35      0.33      0.34       450\n","           4       0.44      0.36      0.40       660\n","           5       0.51      0.55      0.53       630\n","           6       0.73      0.25      0.37       150\n","           7       0.62      0.56      0.59       450\n","           8       0.52      0.37      0.43       450\n","           9       0.74      0.52      0.61       480\n","          10       0.27      0.74      0.40       660\n","          11       0.75      0.68      0.71       420\n","          12       0.59      0.56      0.58       690\n","          13       0.60      0.66      0.63       720\n","          14       0.74      0.56      0.64       270\n","          15       0.51      0.56      0.53       210\n","          16       0.76      0.65      0.70       150\n","          17       0.66      0.64      0.65       360\n","          18       0.46      0.42      0.44       390\n","          19       0.57      0.38      0.46        60\n","          20       0.66      0.54      0.60        90\n","          21       0.36      0.31      0.34        90\n","          22       0.52      0.63      0.57       120\n","          23       0.44      0.38      0.41       150\n","          24       0.13      0.08      0.10        90\n","          25       0.66      0.64      0.65       480\n","          26       0.55      0.57      0.56       180\n","          27       0.53      0.43      0.48        60\n","          28       0.44      0.47      0.45       150\n","          29       0.29      0.28      0.28        90\n","          30       0.38      0.21      0.27       150\n","          31       0.71      0.60      0.65       270\n","          32       0.34      0.32      0.33        60\n","          33       0.36      0.36      0.36       210\n","          34       0.33      0.28      0.30       120\n","          35       0.31      0.27      0.29       390\n","          36       0.37      0.42      0.39       120\n","          37       0.21      0.20      0.21        60\n","          38       0.38      0.31      0.34       690\n","          39       0.21      0.17      0.19        90\n","          40       0.47      0.48      0.47        90\n","          41       0.40      0.35      0.38        60\n","          42       0.42      0.28      0.34        90\n","\n","    accuracy                           0.50     12630\n","   macro avg       0.48      0.44      0.45     12630\n","weighted avg       0.52      0.50      0.50     12630\n","\n","ACCURACY of TRAINING DATA\n","0.8044326557678084\n","-------------------------------------------------------\n","MLP WITH 200 HIDDEN LAYERS\n","[INFO] pre-trained classifier not found. \n"," Training Classifier MLP\n","MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=200, learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[INFO] Succefully trained the classsifier. \n"," Saving the classifier for further use\n","[INFO] Classifier Saved\n","EVALUATION ON TESTING DATA FOR200MLP\n","              precision    recall  f1-score   support\n","\n","           0       0.35      0.27      0.30        60\n","           1       0.61      0.64      0.63       720\n","           2       0.64      0.64      0.64       750\n","           3       0.41      0.34      0.37       450\n","           4       0.25      0.60      0.35       660\n","           5       0.58      0.51      0.54       630\n","           6       0.61      0.29      0.39       150\n","           7       0.54      0.57      0.56       450\n","           8       0.48      0.35      0.41       450\n","           9       0.72      0.56      0.63       480\n","          10       0.66      0.62      0.64       660\n","          11       0.67      0.68      0.67       420\n","          12       0.56      0.55      0.55       690\n","          13       0.62      0.65      0.64       720\n","          14       0.63      0.56      0.59       270\n","          15       0.51      0.54      0.52       210\n","          16       0.81      0.66      0.73       150\n","          17       0.57      0.58      0.58       360\n","          18       0.46      0.39      0.43       390\n","          19       0.44      0.38      0.41        60\n","          20       0.52      0.53      0.53        90\n","          21       0.28      0.23      0.26        90\n","          22       0.55      0.63      0.59       120\n","          23       0.38      0.40      0.39       150\n","          24       0.13      0.09      0.10        90\n","          25       0.64      0.61      0.63       480\n","          26       0.57      0.62      0.60       180\n","          27       0.50      0.35      0.41        60\n","          28       0.41      0.47      0.44       150\n","          29       0.20      0.18      0.19        90\n","          30       0.38      0.29      0.33       150\n","          31       0.72      0.61      0.66       270\n","          32       0.35      0.55      0.43        60\n","          33       0.37      0.32      0.34       210\n","          34       0.39      0.35      0.37       120\n","          35       0.35      0.28      0.31       390\n","          36       0.44      0.47      0.45       120\n","          37       0.18      0.15      0.17        60\n","          38       0.38      0.31      0.34       690\n","          39       0.24      0.14      0.18        90\n","          40       0.45      0.49      0.47        90\n","          41       0.35      0.27      0.30        60\n","          42       0.47      0.30      0.37        90\n","\n","    accuracy                           0.51     12630\n","   macro avg       0.47      0.44      0.45     12630\n","weighted avg       0.52      0.51      0.51     12630\n","\n","ACCURACY of TRAINING DATA\n","0.8810732229845188\n","-------------------------------------------------------\n","MLP WITH 300 HIDDEN LAYERS\n","[INFO] pre-trained classifier not found. \n"," Training Classifier MLP\n","MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=300, learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[INFO] Succefully trained the classsifier. \n"," Saving the classifier for further use\n","[INFO] Classifier Saved\n","EVALUATION ON TESTING DATA FOR300MLP\n","              precision    recall  f1-score   support\n","\n","           0       0.26      0.20      0.23        60\n","           1       0.63      0.61      0.62       720\n","           2       0.63      0.64      0.63       750\n","           3       0.39      0.34      0.37       450\n","           4       0.45      0.34      0.39       660\n","           5       0.56      0.50      0.53       630\n","           6       0.60      0.27      0.37       150\n","           7       0.58      0.56      0.57       450\n","           8       0.46      0.37      0.41       450\n","           9       0.69      0.54      0.61       480\n","          10       0.26      0.70      0.38       660\n","          11       0.74      0.66      0.70       420\n","          12       0.53      0.60      0.56       690\n","          13       0.60      0.62      0.61       720\n","          14       0.68      0.54      0.61       270\n","          15       0.46      0.53      0.50       210\n","          16       0.81      0.66      0.73       150\n","          17       0.59      0.58      0.58       360\n","          18       0.47      0.45      0.46       390\n","          19       0.45      0.43      0.44        60\n","          20       0.57      0.53      0.55        90\n","          21       0.29      0.24      0.26        90\n","          22       0.54      0.62      0.58       120\n","          23       0.37      0.35      0.36       150\n","          24       0.16      0.11      0.13        90\n","          25       0.64      0.61      0.62       480\n","          26       0.59      0.58      0.59       180\n","          27       0.51      0.45      0.48        60\n","          28       0.44      0.46      0.45       150\n","          29       0.24      0.21      0.22        90\n","          30       0.38      0.20      0.26       150\n","          31       0.73      0.60      0.66       270\n","          32       0.25      0.28      0.26        60\n","          33       0.39      0.29      0.33       210\n","          34       0.29      0.25      0.27       120\n","          35       0.29      0.25      0.27       390\n","          36       0.36      0.42      0.39       120\n","          37       0.19      0.17      0.18        60\n","          38       0.42      0.32      0.36       690\n","          39       0.13      0.11      0.12        90\n","          40       0.39      0.48      0.43        90\n","          41       0.32      0.33      0.33        60\n","          42       0.49      0.26      0.34        90\n","\n","    accuracy                           0.49     12630\n","   macro avg       0.46      0.43      0.44     12630\n","weighted avg       0.51      0.49      0.49     12630\n","\n","ACCURACY of TRAINING DATA\n","0.8902037797444464\n","-------------------------------------------------------\n","MLP WITH 500 HIDDEN LAYERS\n","[INFO] pre-trained classifier not found. \n"," Training Classifier MLP\n","MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=500, learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)\n","[INFO] Succefully trained the classsifier. \n"," Saving the classifier for further use\n","[INFO] Classifier Saved\n","EVALUATION ON TESTING DATA FOR500MLP\n","              precision    recall  f1-score   support\n","\n","           0       0.41      0.37      0.39        60\n","           1       0.67      0.58      0.62       720\n","           2       0.59      0.67      0.63       750\n","           3       0.37      0.36      0.36       450\n","           4       0.50      0.32      0.39       660\n","           5       0.57      0.52      0.54       630\n","           6       0.52      0.29      0.37       150\n","           7       0.56      0.56      0.56       450\n","           8       0.52      0.41      0.45       450\n","           9       0.68      0.61      0.64       480\n","          10       0.26      0.65      0.37       660\n","          11       0.72      0.69      0.71       420\n","          12       0.56      0.61      0.59       690\n","          13       0.60      0.68      0.64       720\n","          14       0.67      0.54      0.60       270\n","          15       0.57      0.50      0.54       210\n","          16       0.76      0.66      0.70       150\n","          17       0.67      0.62      0.64       360\n","          18       0.54      0.42      0.47       390\n","          19       0.40      0.38      0.39        60\n","          20       0.54      0.67      0.60        90\n","          21       0.28      0.14      0.19        90\n","          22       0.60      0.59      0.60       120\n","          23       0.51      0.41      0.45       150\n","          24       0.17      0.09      0.12        90\n","          25       0.59      0.67      0.62       480\n","          26       0.56      0.66      0.61       180\n","          27       0.55      0.48      0.51        60\n","          28       0.55      0.43      0.49       150\n","          29       0.29      0.28      0.28        90\n","          30       0.32      0.27      0.29       150\n","          31       0.73      0.61      0.66       270\n","          32       0.21      0.28      0.24        60\n","          33       0.38      0.38      0.38       210\n","          34       0.33      0.23      0.27       120\n","          35       0.30      0.26      0.28       390\n","          36       0.41      0.39      0.40       120\n","          37       0.15      0.12      0.13        60\n","          38       0.43      0.30      0.36       690\n","          39       0.15      0.13      0.14        90\n","          40       0.50      0.50      0.50        90\n","          41       0.38      0.40      0.39        60\n","          42       0.55      0.27      0.36        90\n","\n","    accuracy                           0.51     12630\n","   macro avg       0.48      0.44      0.45     12630\n","weighted avg       0.52      0.51      0.51     12630\n","\n","ACCURACY of TRAINING DATA\n","0.8899997449565151\n","-------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r7ozVMvYJDxC"},"source":[""],"execution_count":null,"outputs":[]}]}